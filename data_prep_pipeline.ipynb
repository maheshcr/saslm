{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SASLM Data Preparation Pipeline\n",
                "\n",
                "This notebook implements the data extraction pipeline for the Sri Aurobindo Small Language Model (SASLM).\n",
                "It uses Tesseract OCR with the `script/Latin` model to accurately capture Romanized Sanskrit diacritics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (Run this if on Colab)\n",
                "!sudo apt-get install tesseract-ocr tesseract-ocr-script-latn poppler-utils\n",
                "!pip install pytesseract pdf2image tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import logging\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "import pytesseract\n",
                "from pdf2image import convert_from_path\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Settings\n",
                "RAW_DATA_DIR = \"raw_data\" # Path to folder containing PDFs\n",
                "OUTPUT_DIR = \"processed_text\"\n",
                "DPI = 300\n",
                "LANG = 'script/Latin'\n",
                "HEADER_CUTOFF_PERCENT = 0.08\n",
                "FOOTER_CUTOFF_PERCENT = 0.08\n",
                "\n",
                "if not os.path.exists(OUTPUT_DIR):\n",
                "    os.makedirs(OUTPUT_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_ocr_text(text):\n",
                "    if not text: return \"\"\n",
                "    lines = text.split('\\n')\n",
                "    cleaned_lines = []\n",
                "    for line in lines:\n",
                "        s = line.strip()\n",
                "        if not s:\n",
                "            continue\n",
                "        # Remove standalone page numbers\n",
                "        if s.isdigit() and len(s) < 4:\n",
                "            continue\n",
                "        cleaned_lines.append(line)\n",
                "    return \"\\n\".join(cleaned_lines)\n",
                "\n",
                "def process_pdf(pdf_path):\n",
                "    filename = os.path.basename(pdf_path)\n",
                "    item_name = os.path.splitext(filename)[0]\n",
                "    output_path = os.path.join(OUTPUT_DIR, f\"{item_name}.txt\")\n",
                "    \n",
                "    if os.path.exists(output_path):\n",
                "        print(f\"Skipping {filename}, already processed.\")\n",
                "        return\n",
                "    \n",
                "    print(f\"Processing {filename}...\")\n",
                "    try:\n",
                "        # For Colab/Notebook, strict page iteration to show progress on single file\n",
                "        # We'll rely on pdf2image convert_from_path generator if available, or chunking\n",
                "        # Simple chunking logic to avoid RAM explosion\n",
                "        full_text = []\n",
                "        # Lazy way: let pdf2image handle it. For 600 pages, it might use 2GB RAM.\n",
                "        # If strictly confined resources, use chunking.\n",
                "        images = convert_from_path(pdf_path, dpi=DPI)\n",
                "        \n",
                "        for img in tqdm(images, desc=f\"Pages in {filename}\"):\n",
                "            w, h = img.size\n",
                "            top = int(h * HEADER_CUTOFF_PERCENT)\n",
                "            bottom = int(h * (1 - FOOTER_CUTOFF_PERCENT))\n",
                "            cropped_img = img.crop((0, top, w, bottom))\n",
                "            \n",
                "            text = pytesseract.image_to_string(cropped_img, lang=LANG)\n",
                "            full_text.append(clean_ocr_text(text))\n",
                "            \n",
                "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(\"\\n\\n\".join(full_text))\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {filename}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run\n",
                "pdf_files = glob.glob(os.path.join(RAW_DATA_DIR, \"*.pdf\"))\n",
                "print(f\"Found {len(pdf_files)} PDFs.\")\n",
                "\n",
                "# For notebook, sequential is better to see progress bars\n",
                "for p in pdf_files:\n",
                "    process_pdf(p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f4cd1493",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
                "import glob\n",
                "import os\n",
                "import logging\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "\n",
                "def train_tokenizer(vocab_size=30000):\n",
                "    \"\"\"\n",
                "    Trains a BPE tokenizer on the .txt files in processed_text/.\n",
                "    \"\"\"\n",
                "    files = glob.glob(\"processed_text/*.txt\")\n",
                "    if not files:\n",
                "        logging.error(\"No text files found in processed_text/. Run extraction pipeline first.\")\n",
                "        return\n",
                "\n",
                "    logging.info(f\"Found {len(files)} files to train on.\")\n",
                "\n",
                "    # Initialize Tokenizer in BPE\n",
                "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
                "    \n",
                "    # Pre-tokenizer: Split by whitespace and punctuation\n",
                "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
                "    \n",
                "    # Decoder: Reconstruct from ByteLevel\n",
                "    tokenizer.decoder = decoders.ByteLevel()\n",
                "    \n",
                "    # Trainer\n",
                "    trainer = trainers.BpeTrainer(\n",
                "        vocab_size=vocab_size, \n",
                "        min_frequency=2,\n",
                "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<|endoftext|>\"],\n",
                "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
                "    )\n",
                "    \n",
                "    # Train\n",
                "    logging.info(\"Starting tokenizer training...\")\n",
                "    tokenizer.train(files, trainer)\n",
                "    \n",
                "    # Post-processor (for BERT-like wrapping if needed, but for GPT usually simple)\n",
                "    # GPT2 uses ByteLevel, no explicit post-processor beyond that usually needed for generation loop.\n",
                "    \n",
                "    # Save\n",
                "    save_path = \"saslm_tokenizer.json\"\n",
                "    tokenizer.save(save_path)\n",
                "    logging.info(f\"Tokenizer saved to {save_path}\")\n",
                "    \n",
                "    # Test\n",
                "    sample = \"The nature of the Supermind is rta-chit.\"\n",
                "    encoded = tokenizer.encode(sample)\n",
                "    logging.info(f\"Test Sentence: '{sample}'\")\n",
                "    logging.info(f\"Tokens: {encoded.tokens}\")\n",
                "    logging.info(f\"IDs: {encoded.ids}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
