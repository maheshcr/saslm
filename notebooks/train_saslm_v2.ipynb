{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SASLM v2 Training Notebook\n",
    "\n",
    "Train the Sri Aurobindo Small Language Model with:\n",
    "- Weighted sampling (mature works prioritized)\n",
    "- Checkpointing to Google Drive (survives disconnects)\n",
    "- Grokking detection\n",
    "- Comprehensive logging\n",
    "\n",
    "## Experiments\n",
    "- **EXP-A1**: From scratch, prose only\n",
    "- **EXP-B1**: Fine-tune GPT-2, prose only\n",
    "- **EXP-A2**: From scratch, prose + poetry\n",
    "- **EXP-B2**: Fine-tune GPT-2, prose + poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for checkpoint persistence)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (or upload files)\n",
    "# Option 1: Clone from GitHub\n",
    "# !git clone https://github.com/YOUR_USERNAME/saslm.git /content/saslm\n",
    "\n",
    "# Option 2: Upload zip and extract\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload saslm.zip\n",
    "# !unzip saslm.zip -d /content/\n",
    "\n",
    "# For now, assume files are in /content/saslm\n",
    "%cd /content/saslm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch transformers tokenizers datasets wandb tqdm pyyaml numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Corpus (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if corpus exists\n",
    "if not os.path.exists('./data/clean_prose/corpus_stats.json'):\n",
    "    print(\"Building prose corpus...\")\n",
    "    !python src/data/build_corpus.py --mode prose --source ./processed_text\n",
    "else:\n",
    "    print(\"Prose corpus already exists\")\n",
    "    !python src/data/build_corpus.py --info --output ./data/clean_prose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Tokenizer (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tokenizer exists\n",
    "if not os.path.exists('./tokenizers/tokenizer_16k/tokenizer.json'):\n",
    "    print(\"Training tokenizer...\")\n",
    "    !python src/data/train_tokenizer.py \\\n",
    "        --corpus ./data/clean_prose \\\n",
    "        --vocab-size 16384 \\\n",
    "        --output ./tokenizers/tokenizer_16k\n",
    "else:\n",
    "    print(\"Tokenizer already exists\")\n",
    "    !python src/data/train_tokenizer.py --analyze ./tokenizers/tokenizer_16k --corpus ./data/clean_prose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose experiment\n",
    "EXPERIMENT = \"EXP-A1\"  # Options: EXP-A1, EXP-B1, EXP-A2, EXP-B2\n",
    "\n",
    "config_map = {\n",
    "    \"EXP-A1\": \"configs/exp_a1_prose_only.yaml\",\n",
    "    \"EXP-B1\": \"configs/exp_b1_prose_only_finetune.yaml\",\n",
    "    \"EXP-A2\": \"configs/exp_a2_prose_poetry.yaml\",\n",
    "    \"EXP-B2\": \"configs/exp_b2_prose_poetry_finetune.yaml\",\n",
    "}\n",
    "\n",
    "CONFIG_PATH = config_map[EXPERIMENT]\n",
    "print(f\"Selected: {EXPERIMENT}\")\n",
    "print(f\"Config: {CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View config\n",
    "!cat {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Training\n",
    "\n",
    "Training will:\n",
    "- Auto-resume from checkpoint if disconnected\n",
    "- Save checkpoints to Google Drive every 1000 steps\n",
    "- Log metrics to wandb (optional)\n",
    "- Detect grokking phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Weights & Biases for tracking\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (will auto-resume if checkpoint exists)\n",
    "!python src/training/train.py \\\n",
    "    --config {CONFIG_PATH} \\\n",
    "    --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and generate samples\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.training.train import GPT\n",
    "from src.training.checkpoint_manager import CheckpointManager\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file('./tokenizers/tokenizer_16k/tokenizer.json')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "# Create model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=512,\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    ")\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint_mgr = CheckpointManager(EXPERIMENT)\n",
    "checkpoint_mgr.load_best(model, device='cuda')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "prompts = [\n",
    "    \"The Supermind is\",\n",
    "    \"The psychic being differs from the soul in that\",\n",
    "    \"The goal of Integral Yoga is not merely liberation but\",\n",
    "    \"In the process of spiritual evolution,\",\n",
    "    \"The three modes of Nature are\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Encode\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([encoded.ids], device='cuda')\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens=100, temperature=0.8, top_k=50)\n",
    "    \n",
    "    # Decode\n",
    "    generated = tokenizer.decode(output[0].tolist())\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run LLM Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key for evaluation (choose one)\n",
    "import os\n",
    "\n",
    "# Option 1: OpenAI\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "# Option 2: Anthropic\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'your-key-here'\n",
    "\n",
    "# Option 3: Google\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-key-here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "# !python src/evaluate.py \\\n",
    "#     --model-path /content/drive/MyDrive/saslm/experiments/{EXPERIMENT}/best_model.pt \\\n",
    "#     --tokenizer ./tokenizers/tokenizer_16k \\\n",
    "#     --judge claude \\\n",
    "#     --output ./results/{EXPERIMENT}_eval.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to HuggingFace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi()\n",
    "# \n",
    "# api.upload_folder(\n",
    "#     folder_path=f'/content/drive/MyDrive/saslm/experiments/{EXPERIMENT}',\n",
    "#     repo_id='your-username/saslm-v2',\n",
    "#     repo_type='model',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load metrics\n",
    "metrics_path = f'/content/drive/MyDrive/saslm/experiments/{EXPERIMENT}/metrics.jsonl'\n",
    "\n",
    "steps = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "with open(metrics_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        steps.append(data['step'])\n",
    "        if 'train_loss' in data:\n",
    "            train_losses.append((data['step'], data['train_loss']))\n",
    "        if 'val_loss' in data:\n",
    "            val_losses.append((data['step'], data['val_loss']))\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Training loss\n",
    "if train_losses:\n",
    "    x, y = zip(*train_losses)\n",
    "    ax1.plot(x, y, label='Train Loss', alpha=0.7)\n",
    "if val_losses:\n",
    "    x, y = zip(*val_losses)\n",
    "    ax1.plot(x, y, label='Val Loss', alpha=0.7)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gap (for grokking detection)\n",
    "if train_losses and val_losses:\n",
    "    # Align by step\n",
    "    train_dict = dict(train_losses)\n",
    "    val_dict = dict(val_losses)\n",
    "    common_steps = sorted(set(train_dict.keys()) & set(val_dict.keys()))\n",
    "    gaps = [val_dict[s] - train_dict[s] for s in common_steps]\n",
    "    ax2.plot(common_steps, gaps, label='Val - Train Gap', color='purple')\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Gap')\n",
    "    ax2.set_title('Generalization Gap (Grokking Indicator)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./results/{EXPERIMENT}_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### If Colab Disconnects\n",
    "Just re-run from the \"Run Training\" cell. The training will automatically resume from the last checkpoint.\n",
    "\n",
    "### Checkpoint Locations\n",
    "- Latest: `/content/drive/MyDrive/saslm/experiments/{EXPERIMENT}/checkpoint_latest.pt`\n",
    "- Best: `/content/drive/MyDrive/saslm/experiments/{EXPERIMENT}/best_model.pt`\n",
    "- Metrics: `/content/drive/MyDrive/saslm/experiments/{EXPERIMENT}/metrics.jsonl`\n",
    "\n",
    "### Expected Training Time\n",
    "- EXP-A1 (from scratch): ~8-12 hours for 100K steps on T4\n",
    "- EXP-B1 (fine-tune): ~4-6 hours for 50K steps on T4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_version": "3.10"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
