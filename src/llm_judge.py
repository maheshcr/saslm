import os
import json
import logging

class LLMJudge:
    def __init__(self, provider="openai", model_name=None):
        self.provider = provider
        self.client = None
        self.model_name = model_name
        
        self.setup_client()

    def setup_client(self):
        if self.provider == "openai":
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
                if not self.model_name: self.model_name = "gpt-4-turbo-preview"
            except ImportError:
                print("Error: 'openai' package not installed.")
            except Exception as e:
                print(f"Error checking OpenAI key: {e}")
                
        elif self.provider == "anthropic":
            try:
                from anthropic import Anthropic
                self.client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
                if not self.model_name: self.model_name = "claude-3-opus-20240229"
            except ImportError:
                print("Error: 'anthropic' package not installed.")

        elif self.provider == "gemini":
            try:
                import google.generativeai as genai
                genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
                self.client = genai
                if not self.model_name: self.model_name = "gemini-1.5-pro-latest"
            except ImportError:
                print("Error: 'google-generativeai' package not installed.")

    def grade_response(self, prompt, generated_text):
        """
        Asks the judge to grade the response based on ontological accuracy, style, and coherence.
        Returns a dict: { "score": int, "components": {...}, "reasoning": str }
        """
        judge_prompt = f"""
You are an expert scholar in the philosophy of Sri Aurobindo (Integral Yoga).
Please evaluate the following text fragment generated by a student model completing a prompt.

PROMPT:
"{prompt}"

GENERATED COMPLETION:
"{generated_text}"

Grade this completion on a scale of 1-10 based on these criteria:
1. Ontological Accuracy: Does it use terms like 'Supermind', 'Psychic Being', 'Inconscient' correctly according to Aurobindo's definitions?
2. Stylistic Fidelity: Does it capture the rhythmic, sentence structure of Aurobindo?
3. Coherence: Is it grammatically correct and logically consistent?

Return your response in PURE JSON format:
{{
  "ontological_accuracy": <score 1-10>,
  "stylistic_fidelity": <score 1-10>,
  "coherence": <score 1-10>,
  "overall_score": <average 1-10>,
  "reasoning": "<brief explanation>"
}}
"""
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant outputting JSON."},
                        {"role": "user", "content": judge_prompt}
                    ],
                    response_format={"type": "json_object"}
                )
                return json.loads(response.choices[0].message.content)

            elif self.provider == "anthropic":
                message = self.client.messages.create(
                    model=self.model_name,
                    max_tokens=1024,
                    messages=[{"role": "user", "content": judge_prompt}]
                )
                # Anthropic doesn't force JSON object mode as easily, might need parsing
                content = message.content[0].text
                # valid json finding?
                start = content.find('{')
                end = content.rfind('}') + 1
                return json.loads(content[start:end])
                
            elif self.provider == "gemini":
                model = self.client.GenerativeModel(self.model_name)
                response = model.generate_content(judge_prompt)
                content = response.text
                start = content.find('{')
                end = content.rfind('}') + 1
                return json.loads(content[start:end])
                
        except Exception as e:
            logging.error(f"Judge verification failed: {e}")
            return {
                "overall_score": 0,
                "reasoning": f"Judge Error: {e}"
            }

if __name__ == "__main__":
    # Test stub
    judge = LLMJudge(provider="openai")
    res = judge.grade_response("The goal of Yoga is", "to eat ice cream.")
    print(res)
